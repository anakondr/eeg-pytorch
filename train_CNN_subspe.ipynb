{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdb3e5f3d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import itertools\n",
    "import pandas as pd \n",
    "import pickle \n",
    "import numpy as np\n",
    "import mne\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddat(filename):        \n",
    "    with open(filename, 'rb') as handle:\n",
    "        b = pickle.load(handle)          \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = dict(BNCI2014004 = 'aBNCI2014004R.pickle',\n",
    "             BNCI2014001 = 'aBNCI2014001R.pickle',\n",
    "             Weibo2014   = 'aWeibo2014R.pickle',\n",
    "             Physionet   = 'aPhysionetRR.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemname is one of : ['BNCI2014004', 'BNCI2014001', 'Weibo2014', 'Physionet']\n",
    "itemname = 'BNCI2014001'\n",
    "filename = dname[itemname]\n",
    "\n",
    "iname = itemname + '__'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loaddat(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['right_hand'].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load subject specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nu_smrutils import subject_specific, augment_dataset, crop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectIndex = list(range(108))\n",
    "class_name = ['left_hand', 'right_hand']\n",
    "\n",
    "datt = subject_specific(data, subjectIndex, class_name, \n",
    "                        normalize = True, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subspe Data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dat in datt:    \n",
    "    print(dat['xtrain'].shape)\n",
    "datt[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment only the training data \n",
    "augmdata = dict(std_dev = 0.01,  multiple = 2)\n",
    "\n",
    "for ii, dat in enumerate(datt):    \n",
    "    xtrain, ytrain = augment_dataset(dat['xtrain'], dat['ytrain'], std_dev = augmdata['std_dev'], \n",
    "                                     multiple = augmdata['multiple'])\n",
    "    print(xtrain.shape)\n",
    "    datt[ii]['xtrain'], datt[ii]['ytrain'] = xtrain, ytrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subspe Data Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fs = 80 # sampling frequency \n",
    "crop_len = 1.5 #or None\n",
    "crop = dict(fs = fs, crop_len = crop_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ii, dat in enumerate(datt):\n",
    "    if crop['crop_len']:\n",
    "       X_train, y_train = crop_data(crop['fs'], crop['crop_len'], dat['xtrain'], dat['ytrain'])\n",
    "       X_valid, y_valid = crop_data(crop['fs'], crop['crop_len'], dat['xvalid'], dat['yvalid'])\n",
    "       X_test,  y_test  = crop_data(crop['fs'], crop['crop_len'], dat['xtest'],  dat['ytest'])\n",
    "       \n",
    "       print(X_train.shape)\n",
    "       datt[ii] = dict(xtrain = X_train, xvalid = X_valid, xtest = X_test,\n",
    "                       ytrain = y_train, yvalid = y_valid, ytest = y_test)                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader  \n",
    "\n",
    "def get_data_loaders(dat, batch_size, EEGNET = None):    \n",
    "    # convert data dimensions to into to gray scale image format\n",
    "    if EEGNET: ### EEGNet model requires the last dimension to be 1 \n",
    "        ff = lambda dat: torch.unsqueeze(dat, dim = -1)    \n",
    "    else:\n",
    "        ff = lambda dat: torch.unsqueeze(dat, dim = 1)    \n",
    "        \n",
    "    x_train, x_valid, x_test =  map(ff,(dat['xtrain'],dat['xvalid'],dat['xtest']))    \n",
    "    y_train, y_valid, y_test =  dat['ytrain'], dat['yvalid'], dat['ytest']\n",
    "    print('Input data shape', x_train.shape)       \n",
    "    \n",
    "    # TensorDataset & Dataloader    \n",
    "    train_dat = TensorDataset(x_train, y_train) \n",
    "    val_dat   = TensorDataset(x_valid, y_valid) \n",
    "    \n",
    "    train_loader = DataLoader(train_dat, batch_size = batch_size, shuffle = True)\n",
    "    val_loader   = DataLoader(val_dat,   batch_size = batch_size, shuffle = False)\n",
    "\n",
    "    output = dict(dset_loaders = {'train': train_loader, 'val': val_loader}, \n",
    "                  dset_sizes  =  {'train': len(x_train), 'val': len(x_valid)},\n",
    "                  test_data   =  {'x_test' : x_test, 'y_test' : y_test})          \n",
    "    return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subspe dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii, dat in enumerate(datt): #for each dataset \n",
    "    datt[ii] = get_data_loaders(dat, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datt[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CNN2D(torch.nn.Module):  \n",
    "    def __init__(self, input_size, kernel_size, conv_channels, dense_size, dropout):         \n",
    "        super(CNN2D, self).__init__()        \n",
    "        self.cconv   = []  \n",
    "        self.MaxPool = nn.MaxPool2d((1, 2), (1, 2))  \n",
    "        self.ReLU    = nn.ReLU()\n",
    "        self.Dropout = nn.Dropout(dropout)        \n",
    "        self.batchnorm = []        \n",
    "        # ############ batchnorm ###########\n",
    "        for jj in conv_channels:\n",
    "            self.batchnorm.append(nn.BatchNorm2d(jj, eps=0.001, momentum=0.01,\n",
    "                                                 affine=True, track_running_stats=True).cuda())         \n",
    "        # ############ define CONV layer architecture: #########\n",
    "        ii = 0\n",
    "        for in_channels, out_channels in zip(conv_channels, conv_channels[1:]):                           \n",
    "            conv_i = torch.nn.Conv2d(in_channels  = in_channels, out_channels = out_channels,\n",
    "                                     kernel_size  = kernel_size[ii], #stride = (1, 2),\n",
    "                                     padding      = (kernel_size[ii][0]//2, kernel_size[ii][1]//2))\n",
    "            \n",
    "            self.cconv.append(conv_i)                \n",
    "            self.add_module('CNN_K{}_O{}'.format(kernel_size[ii], out_channels), conv_i)\n",
    "            ii += 1 \n",
    "        ##########################################################    \n",
    "        self.flat_dim = self.get_output_dim(input_size, self.cconv)    \n",
    "        self.fc1 = torch.nn.Linear(self.flat_dim, dense_size)\n",
    "        self.fc2 = torch.nn.Linear(dense_size, 2)     \n",
    "\n",
    "    def get_output_dim(self, input_size, cconv):        \n",
    "        with torch.no_grad():\n",
    "            input = torch.ones(1,*input_size)              \n",
    "            for conv_i in cconv:                \n",
    "                input = self.MaxPool(conv_i(input))        \n",
    "                flatout = int(np.prod(input.size()[1:]))\n",
    "                print(\"Flattened output ::\", flatout, input.shape)                \n",
    "        return flatout \n",
    "        \n",
    "    def forward(self, input):        \n",
    "        for jj, conv_i in enumerate(self.cconv):\n",
    "            #conv_i.cuda()            \n",
    "            input = conv_i(input)\n",
    "            input = self.batchnorm[jj+1](input)\n",
    "            input = self.ReLU(input)        \n",
    "            input = self.MaxPool(input)                   \n",
    "        # flatten the CNN output     \n",
    "        out = input.view(-1, self.flat_dim) \n",
    "        out = self.fc1(out)                       \n",
    "        out = self.Dropout(out)        \n",
    "        out = self.fc2(out)      \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your GPU device name : GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "print('Your GPU device name :', torch.cuda.get_device_name())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nu_train_utils import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0c085d6150aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# define kernel size in terms of ms length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtimE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;31m#ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimE\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# ker = 8 #timelength//chans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fs' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 150 \n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4  \n",
    "batch_size = 64\n",
    "verbose = 2\n",
    "\n",
    "# define kernel size in terms of ms length \n",
    "timE = 100 #ms\n",
    "width = timE*fs//1000    \n",
    "\n",
    "# ker = 8 #timelength//chans \n",
    "h, w = 3, 1  #hight and width of a rectangular kernel      \n",
    "\n",
    "if itemname == 'BNCI2014001':\n",
    "        kernel_size = [(h, w*width), (h, w*width), (h, w*width),(h, w*width),(h, w*width),(h, w*width)]\n",
    "        conv_chan   = [1, 64, 32, 16, 8] \n",
    "elif itemname == 'Weibo2014':\n",
    "        kernel_size = [(h, w*width), (h, w*width), (h, w*width),(h, w*width),(h, w*width),(h, w*width)]\n",
    "        conv_chan   = [1, 64, 32, 16, 8]            \n",
    "elif itemname == 'Physionet':\n",
    "        kernel_size = [(h, w*width), (h, w*width), (h, w*width),(h, w*width),(h, w*width),(h, w*width)]\n",
    "        conv_chan   = [1, 8, 16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% used to save the results table \n",
    "results = {}        \n",
    "table = pd.DataFrame(columns = ['Train_Acc', 'Val_Acc', 'Test_Acc', 'Epoch']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects : 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of subjects :\", len(datt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 22, 180)\n"
     ]
    }
   ],
   "source": [
    "subject = 4\n",
    "description = 'Subject'+str(subject+1)\n",
    "\n",
    "dat = datt[subject]\n",
    "\n",
    "#% get input size (channel x timepoints)\n",
    "input_size = (1, dat['test_data']['x_test'].shape[-2], \n",
    "                 dat['test_data']['x_test'].shape[-1])\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 22, 90])\n",
      "Flattened output :: 126720\n",
      "torch.Size([1, 32, 22, 45])\n",
      "Flattened output :: 31680\n",
      "torch.Size([1, 16, 22, 23])\n",
      "Flattened output :: 8096\n",
      "torch.Size([1, 8, 22, 12])\n",
      "Flattened output :: 2112\n",
      "Model architecture >>> CNN2D(\n",
      "  (MaxPool): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (ReLU): ReLU()\n",
      "  (Dropout): Dropout(p=0.5, inplace=False)\n",
      "  (CNN_K(3, 8)_O64): Conv2d(1, 64, kernel_size=(3, 8), stride=(1, 1), padding=(1, 4))\n",
      "  (CNN_K(3, 8)_O32): Conv2d(64, 32, kernel_size=(3, 8), stride=(1, 1), padding=(1, 4))\n",
      "  (CNN_K(3, 8)_O16): Conv2d(32, 16, kernel_size=(3, 8), stride=(1, 1), padding=(1, 4))\n",
      "  (CNN_K(3, 8)_O8): Conv2d(16, 8, kernel_size=(3, 8), stride=(1, 1), padding=(1, 4))\n",
      "  (fc1): Linear(in_features=2112, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the architecture\n",
    "model = CNN2D(input_size    = input_size, \n",
    "              kernel_size   = kernel_size, \n",
    "              conv_channels = conv_chan,\n",
    "              dense_size    = 256, \n",
    "              dropout       = 0.5)               \n",
    "\n",
    "# optimizer and the loss function definition \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.to(dev)  \n",
    "criterion.to(dev)       \n",
    "\n",
    "print(\"Model architecture >>>\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "train loss: 0.0312, acc: 0.4795\n",
      "val loss: 0.0180, acc: 0.4796\n",
      "Epoch 2/150\n",
      "train loss: 0.0168, acc: 0.5410\n",
      "val loss: 0.0175, acc: 0.4694\n",
      "Epoch 3/150\n",
      "train loss: 0.0129, acc: 0.5615\n",
      "val loss: 0.0159, acc: 0.5000\n",
      "Epoch 4/150\n",
      "train loss: 0.0115, acc: 0.6103\n",
      "val loss: 0.0168, acc: 0.4796\n",
      "Epoch 5/150\n",
      "train loss: 0.0100, acc: 0.6667\n",
      "val loss: 0.0166, acc: 0.5204\n",
      "Epoch 6/150\n",
      "train loss: 0.0095, acc: 0.7192\n",
      "val loss: 0.0180, acc: 0.5000\n",
      "Epoch 7/150\n",
      "train loss: 0.0083, acc: 0.7782\n",
      "val loss: 0.0203, acc: 0.5000\n",
      "Epoch 8/150\n",
      "train loss: 0.0056, acc: 0.8526\n",
      "val loss: 0.0244, acc: 0.4898\n",
      "Epoch 9/150\n",
      "train loss: 0.0034, acc: 0.9231\n",
      "val loss: 0.0315, acc: 0.5000\n",
      "Epoch 10/150\n",
      "train loss: 0.0022, acc: 0.9590\n",
      "val loss: 0.0293, acc: 0.5204\n",
      "Epoch 11/150\n",
      "train loss: 0.0027, acc: 0.9346\n",
      "val loss: 0.0342, acc: 0.5000\n",
      "Epoch 12/150\n",
      "train loss: 0.0013, acc: 0.9744\n",
      "val loss: 0.0401, acc: 0.5102\n",
      "Epoch 13/150\n",
      "train loss: 0.0006, acc: 0.9910\n",
      "val loss: 0.0453, acc: 0.5204\n",
      "Epoch 14/150\n",
      "train loss: 0.0002, acc: 0.9987\n",
      "val loss: 0.0464, acc: 0.5000\n",
      "Epoch 15/150\n",
      "train loss: 0.0001, acc: 1.0000\n",
      "val loss: 0.0481, acc: 0.4898\n",
      "Epoch 16/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0505, acc: 0.4694\n",
      "Epoch 17/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0508, acc: 0.4796\n",
      "Epoch 18/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0512, acc: 0.4796\n",
      "Epoch 19/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0522, acc: 0.4898\n",
      "Epoch 20/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0530, acc: 0.4898\n",
      "Epoch 21/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0532, acc: 0.4898\n",
      "Epoch 22/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0535, acc: 0.4898\n",
      "Epoch 23/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0534, acc: 0.4694\n",
      "Epoch 24/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0539, acc: 0.4694\n",
      "Epoch 25/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0548, acc: 0.4898\n",
      "Epoch 26/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0552, acc: 0.4898\n",
      "Epoch 27/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0558, acc: 0.4796\n",
      "Epoch 28/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0559, acc: 0.4796\n",
      "Epoch 29/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0561, acc: 0.4796\n",
      "Epoch 30/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0563, acc: 0.4796\n",
      "Epoch 31/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0561, acc: 0.4592\n",
      "Epoch 32/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0561, acc: 0.4592\n",
      "Epoch 33/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0567, acc: 0.4592\n",
      "Epoch 34/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0570, acc: 0.4694\n",
      "Epoch 35/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0572, acc: 0.4796\n",
      "Epoch 36/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0573, acc: 0.4694\n",
      "Epoch 37/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0571, acc: 0.4592\n",
      "Epoch 38/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0573, acc: 0.4592\n",
      "Epoch 39/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0577, acc: 0.4592\n",
      "Epoch 40/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0580, acc: 0.4796\n",
      "Epoch 41/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0581, acc: 0.4796\n",
      "Epoch 42/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0581, acc: 0.4796\n",
      "Epoch 43/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0580, acc: 0.4796\n",
      "Epoch 44/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0580, acc: 0.4898\n",
      "Epoch 45/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0582, acc: 0.5000\n",
      "Epoch 46/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0583, acc: 0.5000\n",
      "Epoch 47/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0585, acc: 0.5000\n",
      "Epoch 48/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0585, acc: 0.5000\n",
      "Epoch 49/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0587, acc: 0.4898\n",
      "Epoch 50/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0588, acc: 0.4898\n",
      "Epoch 51/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0589, acc: 0.4898\n",
      "Epoch 52/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0590, acc: 0.4898\n",
      "Epoch 53/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0591, acc: 0.4898\n",
      "Epoch 54/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0592, acc: 0.4898\n",
      "Epoch 55/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0592, acc: 0.4898\n",
      "Epoch 56/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0592, acc: 0.4796\n",
      "Epoch 57/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0592, acc: 0.4694\n",
      "Epoch 58/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0593, acc: 0.4694\n",
      "Epoch 59/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0595, acc: 0.4796\n",
      "Epoch 60/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0597, acc: 0.4796\n",
      "Epoch 61/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0598, acc: 0.4796\n",
      "Epoch 62/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0599, acc: 0.4898\n",
      "Epoch 63/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4796\n",
      "Epoch 64/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0597, acc: 0.4796\n",
      "Epoch 65/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0595, acc: 0.4796\n",
      "Epoch 66/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0594, acc: 0.4796\n",
      "Epoch 67/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0595, acc: 0.4796\n",
      "Epoch 68/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0597, acc: 0.4796\n",
      "Epoch 69/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0598, acc: 0.4694\n",
      "Epoch 70/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0597, acc: 0.4694\n",
      "Epoch 71/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0597, acc: 0.4796\n",
      "Epoch 72/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0601, acc: 0.5000\n",
      "Epoch 73/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0601, acc: 0.5000\n",
      "Epoch 74/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.5000\n",
      "Epoch 75/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0599, acc: 0.4898\n",
      "Epoch 76/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0599, acc: 0.4898\n",
      "Epoch 77/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0598, acc: 0.4898\n",
      "Epoch 78/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0598, acc: 0.4898\n",
      "Epoch 79/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0598, acc: 0.4898\n",
      "Epoch 80/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0599, acc: 0.4898\n",
      "Epoch 81/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4898\n",
      "Epoch 82/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4898\n",
      "Epoch 83/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0601, acc: 0.4796\n",
      "Epoch 84/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0602, acc: 0.4796\n",
      "Epoch 85/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0602, acc: 0.4796\n",
      "Epoch 86/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0601, acc: 0.4796\n",
      "Epoch 87/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0601, acc: 0.4796\n",
      "Epoch 88/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0601, acc: 0.4796\n",
      "Epoch 89/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0602, acc: 0.4796\n",
      "Epoch 90/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0603, acc: 0.4796\n",
      "Epoch 91/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0597, acc: 0.4898\n",
      "Epoch 92/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0597, acc: 0.4898\n",
      "Epoch 93/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0599, acc: 0.4796\n",
      "Epoch 94/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0603, acc: 0.4796\n",
      "Epoch 95/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0604, acc: 0.4796\n",
      "Epoch 96/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4796\n",
      "Epoch 97/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0599, acc: 0.4796\n",
      "Epoch 98/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4796\n",
      "Epoch 99/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0602, acc: 0.4796\n",
      "Epoch 100/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0602, acc: 0.4796\n",
      "Epoch 101/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0602, acc: 0.4796\n",
      "Epoch 102/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0602, acc: 0.4796\n",
      "Epoch 103/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0603, acc: 0.4796\n",
      "Epoch 104/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0603, acc: 0.4796\n",
      "Epoch 105/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0603, acc: 0.4796\n",
      "Epoch 106/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0603, acc: 0.4796\n",
      "Epoch 107/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0605, acc: 0.4796\n",
      "Epoch 108/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0604, acc: 0.4796\n",
      "Epoch 109/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0601, acc: 0.4796\n",
      "Epoch 110/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4694\n",
      "Epoch 111/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4694\n",
      "Epoch 112/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4694\n",
      "Epoch 113/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0602, acc: 0.4796\n",
      "Epoch 114/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0605, acc: 0.4796\n",
      "Epoch 115/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0606, acc: 0.4796\n",
      "Epoch 116/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0607, acc: 0.4796\n",
      "Epoch 117/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0605, acc: 0.4796\n",
      "Epoch 118/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0603, acc: 0.4796\n",
      "Epoch 119/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0604, acc: 0.4796\n",
      "Epoch 120/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0603, acc: 0.4796\n",
      "Epoch 121/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0603, acc: 0.4796\n",
      "Epoch 122/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0602, acc: 0.4796\n",
      "Epoch 123/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4796\n",
      "Epoch 124/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4796\n",
      "Epoch 125/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0599, acc: 0.4796\n",
      "Epoch 126/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0599, acc: 0.4796\n",
      "Epoch 127/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0601, acc: 0.4796\n",
      "Epoch 128/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0607, acc: 0.4796\n",
      "Epoch 129/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0605, acc: 0.4796\n",
      "Epoch 130/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0603, acc: 0.4796\n",
      "Epoch 131/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0601, acc: 0.4796\n",
      "Epoch 132/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0599, acc: 0.4796\n",
      "Epoch 133/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0597, acc: 0.4796\n",
      "Epoch 134/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0597, acc: 0.4796\n",
      "Epoch 135/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0598, acc: 0.4694\n",
      "Epoch 136/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4898\n",
      "Epoch 137/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4796\n",
      "Epoch 138/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4796\n",
      "Epoch 139/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0599, acc: 0.4796\n",
      "Epoch 140/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0600, acc: 0.4796\n",
      "Epoch 141/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0601, acc: 0.4796\n",
      "Epoch 142/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0602, acc: 0.4796\n",
      "Epoch 143/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0602, acc: 0.4796\n",
      "Epoch 144/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0601, acc: 0.4796\n",
      "Epoch 145/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0599, acc: 0.4796\n",
      "Epoch 146/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0598, acc: 0.4796\n",
      "Epoch 147/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0597, acc: 0.4796\n",
      "Epoch 148/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0595, acc: 0.4694\n",
      "Epoch 149/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0605, acc: 0.4796\n",
      "Epoch 150/150\n",
      "train loss: 0.0000, acc: 1.0000\n",
      "val loss: 0.0607, acc: 0.4898\n",
      "Training complete in 3m 27s\n",
      "Best val Acc: 0.520408\n",
      "Best Epoch : 5\n",
      "torch.Size([50, 1, 22, 180])\n",
      "Test Accuracy : 0.66\n",
      "          Train_Acc   Val_Acc  Test_Acc  Epoch\n",
      "Subject5   0.666667  0.520408      0.66    5.0\n"
     ]
    }
   ],
   "source": [
    "#******** Training loop *********    \n",
    "best_model, train_losses, val_losses, train_accs, val_accs, info =\\\n",
    "    train_model(model, dat['dset_loaders'], dat['dset_sizes'], \n",
    "                criterion, optimizer, dev, lr_scheduler = None, num_epochs = num_epochs, verbose = verbose)    \n",
    "\n",
    "test_samples = 50\n",
    "x_test = dat['test_data']['x_test'][:test_samples,:,:,:] \n",
    "y_test = dat['test_data']['y_test'][:test_samples] \n",
    "print(x_test.shape)\n",
    "\n",
    "# predict test data \n",
    "preds = best_model(x_test.to(dev)) \n",
    "preds_class = preds.data.max(1)[1]\n",
    "\n",
    "# get the accuracy \n",
    "corrects = torch.sum(preds_class == y_test.data.to(dev))     \n",
    "test_acc = corrects.cpu().numpy()/x_test.shape[0]\n",
    "print(\"Test Accuracy :\", test_acc) \n",
    "\n",
    "# save results       \n",
    "tab = dict(Train_Acc= train_accs[info['best_epoch']],\n",
    "           Val_Acc  = val_accs[info['best_epoch']],   \n",
    "           Test_Acc = test_acc, Epoch = info['best_epoch'] + 1)         \n",
    "\n",
    "table.loc[description] = tab  \n",
    "val_acc = np.max(val_accs)\n",
    "\n",
    "print(table)\n",
    "results[description] = dict(train_accs = train_accs, val_accs =  val_accs,                                \n",
    "                            ytrain = info['ytrain'], yval= info['yval'])      \n",
    "\n",
    "fname = iname + 'CNN_POOLED' + description + '_' + str(val_acc)[:4]\n",
    "torch.save(best_model.state_dict(), fname) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all the results in one file \n",
    "rTable = dict(table = table)\n",
    "ij = str(np.random.randint(101))\n",
    "filename = iname + \"_CNNSUBSPERES_\"+description +ij+ itemname        \n",
    "\n",
    "with open(filename, 'wb') as ffile:\n",
    "    pickle.dump(rTable, ffile)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BNCI2014001___CNNSUBSPERES_Subject542BNCI2014001'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('mne': conda)",
   "language": "python",
   "name": "python37764bitmneconda21dd67b97d6c49d7b87e308936d73604"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
